# ğŸ“„ PDF Educational Data Extractor with LLMs (Gemma3, LLaMA3, Mistral) + Qwen3 MCQ Generator

Este projeto Ã© uma aplicaÃ§Ã£o em Python com interface Gradio que permite **processar PDFs educacionais**, extrair e estruturar dados com modelos LLM locais via **Ollama**, e **gerar questÃµes de mÃºltipla escolha (MCQs)** com o modelo Qwen3.

---

## ğŸš€ Funcionalidades

- ğŸ“¥ Upload de arquivos PDF
- ğŸ§  ExtraÃ§Ã£o e estruturaÃ§Ã£o de dados com:
  - ğŸ”¹ Gemma3
  - ğŸ”¹ LLaMA 3
  - ğŸ”¹ Mistral
- ğŸ’¾ Salvamento de arquivos JSON com as saÃ­das dos modelos
- ğŸ“ GeraÃ§Ã£o automÃ¡tica de questÃµes de mÃºltipla escolha com o modelo Qwen3
- â¬‡ï¸ Download dos arquivos JSON e TXT processados

-
ğŸ› ï¸ Requisitos

    Python 3.10+

    Ollama

    Modelos:

        Gemma3

        llama3

        mistral

        Qwen3 ou similar para MCQ

    DependÃªncias:

        gradio

        PyMuPDF (fitz)

        json

        os, uuid, datetime

        langchain

â–¶ï¸ Como Executar

  Clone o repositÃ³rio:

    git clone https://github.com/juliacoit/Local-Gradio-App-for-RAG
    cd Local-Gradio-App-for-RAG

Instale as dependÃªncias:

    pip install -r requirements.txt

Certifique-se de que o Ollama esteja rodando com os modelos baixados:

    ollama run gamma3
    ollama run llama3
    ollama run mistral

Inicie o Ollama e Execute o aplicativo:

    ollama serve
    python app.py

ğŸ“ Estrutura do Projeto

.
â”œâ”€â”€ app.py                # Interface principal com Gradio
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ json/             # Arquivos JSON gerados pelos modelos
â”‚   â””â”€â”€ txt/              # Arquivos de texto extraÃ­dos
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸ“Œ ObservaÃ§Ãµes

VocÃª pode adaptar os prompts individualmente para cada modelo no diretÃ³rio prompts/.

Os arquivos sÃ£o salvos com nomes Ãºnicos para facilitar a organizaÃ§Ã£o e o download.
