üß† Local RAG App com Gradio + Ollama + PDF

Este projeto √© uma aplica√ß√£o local para extra√ß√£o de dados educacionais estruturados a partir de arquivos PDF, utilizando modelos LLM hospedados localmente via Ollama, com interface feita em Gradio. O sistema tamb√©m gera automaticamente quest√µes de m√∫ltipla escolha com base no conte√∫do extra√≠do.
üöÄ Funcionalidades

    Extra√ß√£o de texto limpo a partir de arquivos PDF.

    Estrutura√ß√£o inicial do conte√∫do (t√≠tulo, autor, se√ß√µes).

    Uso de modelos LLM locais (phi3:mini, llama3, mistral) para gerar JSON estruturado.

    Corre√ß√£o autom√°tica de erros de formata√ß√£o JSON com json-repair.

    Gera√ß√£o de quest√µes educacionais com o modelo Deepseek-R1.

    Salvamento autom√°tico das sa√≠das (arquivos .json e .txt).

üõ†Ô∏è Requisitos

    Python 3.10+

    Ollama instalado e rodando localmente

    Modelos LLM baixados via Ollama:

        phi3:mini

        llama3

        mistral

        deepseek-r1:8b

    Instalar as depend√™ncias Python:

pip install gradio langchain-community pymupdf json-repair

üß± Estrutura do Projeto

üìÅ resultados/         ‚Üê arquivos gerados (.json e .txt)
üìÑ app.py              ‚Üê script principal com a interface
üìÑ template.json       ‚Üê modelo de estrutura esperado para os dados extra√≠dos

‚ñ∂Ô∏è Como Executar

Inicie o Ollama e certifique-se de que os modelos est√£o dispon√≠veis localmente:

    ollama run phi3:mini
    ollama run llama3
    ollama run mistral
    ollama run deepseek-r1:8b

Execute o script principal:

    python app.py

A interface ser√° carregada via Gradio. Fa√ßa upload de um PDF educacional e aguarde o processamento.

üß† Modelos Suportados
Modelo	Finalidade
phi3:mini	Extra√ß√£o estruturada de texto para JSON
llama3	Mesma fun√ß√£o, com outro modelo LLM
mistral	Alternativa de modelo para extra√ß√£o estruturada
deepseek-r1	Gera√ß√£o de quest√µes com base no conte√∫do
üß™ Pipeline de Processamento

Extra√ß√£o de texto: Convers√£o do PDF para texto puro.

Estrutura√ß√£o inicial: Estimativa de t√≠tulo, autor e se√ß√µes.

Chunking: Divis√£o do texto em blocos com sobreposi√ß√£o.

Gera√ß√£o de JSON estruturado: Cada chunk √© processado por um LLM via Ollama.

Corre√ß√£o de JSON: Heur√≠sticas + json-repair para garantir estrutura v√°lida.

Gera√ß√£o de quest√µes: Com o modelo Deepseek, baseado no conte√∫do extra√≠do.

üìÇ Sa√≠das

Os arquivos gerados s√£o salvos na pasta resultados/ com nomes no formato:

    NOME_DO_PDF_saida_phi3_mini.json
    NOME_DO_PDF_questoes_phi3_mini.txt

üìÑ Modelo de JSON Estruturado

O conte√∫do extra√≠do √© salvo com a seguinte estrutura:

    {
      "titulo": "string",
      "autor": "string",
      "disciplina": "string",
      "data": "string",
      "ementa": ["string"],
      "topicos": [
        {
          "secao": "string",
          "subsecoes": [
            {
              "titulo": "string",
              "conteudo": ["string"]
            }
          ],
          "conteudo": ["string"]
        }
      ],
      "exemplos": [
        {
          "descricao": "string",
          "codigo": "string",
          "explicacao": "string"
        }
      ],
      "referencias": ["string"]
    }

üí° Observa√ß√µes

O template utilizado pelos modelos est√° em template.json.

Verifique se o caminho para o template.json est√° correto no c√≥digo.

A l√≥gica pode ser adaptada para processar outros tipos de conte√∫do al√©m do educacional.
