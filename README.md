ğŸ§  Local RAG App com Gradio + Ollama + PDF

Este projeto Ã© uma aplicaÃ§Ã£o local para extraÃ§Ã£o de dados educacionais estruturados a partir de arquivos PDF, utilizando modelos LLM hospedados localmente via Ollama, com interface feita em Gradio. O sistema tambÃ©m gera automaticamente questÃµes de mÃºltipla escolha com base no conteÃºdo extraÃ­do.
ğŸš€ Funcionalidades

    ExtraÃ§Ã£o de texto limpo a partir de arquivos PDF.

    EstruturaÃ§Ã£o inicial do conteÃºdo (tÃ­tulo, autor, seÃ§Ãµes).

    Uso de modelos LLM locais (phi3:mini, llama3, mistral) para gerar JSON estruturado.

    CorreÃ§Ã£o automÃ¡tica de erros de formataÃ§Ã£o JSON com json-repair.

    GeraÃ§Ã£o de questÃµes educacionais com o modelo Deepseek-R1.

    Salvamento automÃ¡tico das saÃ­das (arquivos .json e .txt).

ğŸ› ï¸ Requisitos

    Python 3.10+

    Ollama instalado e rodando localmente

    Modelos LLM baixados via Ollama:

        phi3:mini

        llama3

        mistral

        deepseek-r1:8b

    Instalar as dependÃªncias Python:

pip install gradio langchain-community pymupdf json-repair

ğŸ§± Estrutura do Projeto

ğŸ“ resultados/         â† arquivos gerados (.json e .txt)
ğŸ“„ app.py              â† script principal com a interface
ğŸ“„ template.json       â† modelo de estrutura esperado para os dados extraÃ­dos

â–¶ï¸ Como Executar

    Inicie o Ollama e certifique-se de que os modelos estÃ£o disponÃ­veis localmente:

ollama run phi3:mini
ollama run llama3
ollama run mistral
ollama run deepseek-r1:8b

    Execute o script principal:

python app.py

    A interface serÃ¡ carregada via Gradio. FaÃ§a upload de um PDF educacional e aguarde o processamento.

ğŸ§  Modelos Suportados
Modelo	Finalidade
phi3:mini	ExtraÃ§Ã£o estruturada de texto para JSON
llama3	Mesma funÃ§Ã£o, com outro modelo LLM
mistral	Alternativa de modelo para extraÃ§Ã£o estruturada
deepseek-r1	GeraÃ§Ã£o de questÃµes com base no conteÃºdo
ğŸ§ª Pipeline de Processamento

    ExtraÃ§Ã£o de texto: ConversÃ£o do PDF para texto puro.

    EstruturaÃ§Ã£o inicial: Estimativa de tÃ­tulo, autor e seÃ§Ãµes.

    Chunking: DivisÃ£o do texto em blocos com sobreposiÃ§Ã£o.

    GeraÃ§Ã£o de JSON estruturado: Cada chunk Ã© processado por um LLM via Ollama.

    CorreÃ§Ã£o de JSON: HeurÃ­sticas + json-repair para garantir estrutura vÃ¡lida.

    GeraÃ§Ã£o de questÃµes: Com o modelo Deepseek, baseado no conteÃºdo extraÃ­do.

ğŸ“‚ SaÃ­das

Os arquivos gerados sÃ£o salvos na pasta resultados/ com nomes no formato:

NOME_DO_PDF_saida_phi3_mini.json
NOME_DO_PDF_questoes_phi3_mini.txt

ğŸ“„ Modelo de JSON Estruturado

O conteÃºdo extraÃ­do Ã© salvo com a seguinte estrutura:

{
  "titulo": "string",
  "autor": "string",
  "disciplina": "string",
  "data": "string",
  "ementa": ["string"],
  "topicos": [
    {
      "secao": "string",
      "subsecoes": [
        {
          "titulo": "string",
          "conteudo": ["string"]
        }
      ],
      "conteudo": ["string"]
    }
  ],
  "exemplos": [
    {
      "descricao": "string",
      "codigo": "string",
      "explicacao": "string"
    }
  ],
  "referencias": ["string"]
}

ğŸ’¡ ObservaÃ§Ãµes

    O template utilizado pelos modelos estÃ¡ em template.json.

    Verifique se o caminho para o template.json estÃ¡ correto no cÃ³digo.

    A lÃ³gica pode ser adaptada para processar outros tipos de conteÃºdo alÃ©m do educacional.
