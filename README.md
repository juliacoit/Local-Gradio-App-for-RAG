# ğŸ§  Local RAG App com Gradio + Ollama + PDF

Este projeto Ã© uma aplicaÃ§Ã£o local para extraÃ§Ã£o de dados educacionais estruturados a partir de arquivos PDF, utilizando modelos LLM hospedados localmente via [Ollama](https://ollama.com/), com interface feita em [Gradio](https://gradio.app/). O sistema tambÃ©m gera automaticamente questÃµes de mÃºltipla escolha com base no conteÃºdo extraÃ­do.

---

## ğŸš€ Funcionalidades

- ExtraÃ§Ã£o de texto limpo a partir de arquivos PDF.
- EstruturaÃ§Ã£o inicial do conteÃºdo (tÃ­tulo, autor, seÃ§Ãµes).
- Uso de modelos LLM locais (Phi-3, LLaMA 3, Mistral) para gerar JSON estruturado.
- CorreÃ§Ã£o automÃ¡tica de erros de formataÃ§Ã£o JSON usando `json-repair`.
- GeraÃ§Ã£o de questÃµes educacionais com o modelo **Deepseek-R1**.
- Salvamento automÃ¡tico de saÃ­das (JSON e TXT) por modelo.

---

## ğŸ› ï¸ Requisitos

- Python 3.10+
- [Ollama](https://ollama.com) instalado e rodando localmente
- Modelos LLM baixados no Ollama (`phi3:mini`, `llama3`, `mistral`, `deepseek-r1:8b`)
- DependÃªncias Python:

```bash
pip install gradio langchain-community pymupdf json-repair
```
ğŸ§± Estrutura do Projeto

ğŸ“ resultados/
    â””â”€ arquivos gerados (JSON e TXT)
ğŸ“„ app.py
ğŸ“„ template.json  â† modelo de estrutura esperado para os dados extraÃ­dos

â–¶ï¸ Como Executar

    Execute o Ollama e certifique-se de que os modelos estÃ£o disponÃ­veis localmente:

ollama run phi3:mini
ollama run llama3
ollama run mistral
ollama run deepseek-r1:8b

    Execute o script principal com:

python app.py

    A interface serÃ¡ carregada via Gradio. Carregue um PDF educacional e aguarde o processamento.

ğŸ§  Modelos Suportados
Nome	Finalidade
phi3:mini	ExtraÃ§Ã£o estruturada de texto para JSON
llama3	Mesma funÃ§Ã£o com outro modelo de LLM
mistral	Idem
deepseek	GeraÃ§Ã£o de questÃµes com base no conteÃºdo
ğŸ§ª Pipeline de Processamento

    ExtraÃ§Ã£o de texto: ConversÃ£o de PDF para texto puro.

    EstruturaÃ§Ã£o inicial: TÃ­tulo, autor e seÃ§Ãµes sÃ£o estimados.

    Chunking: DivisÃ£o do texto em blocos com sobreposiÃ§Ã£o.

    GeraÃ§Ã£o de JSON estruturado: Cada chunk Ã© processado por um LLM via Ollama.

    CorreÃ§Ã£o de JSON: AplicaÃ§Ã£o de heurÃ­sticas + json-repair.

    GeraÃ§Ã£o de questÃµes: Utilizando modelo Deepseek com base no conteÃºdo.

ğŸ“‚ SaÃ­das

Os arquivos gerados ficam na pasta resultados/ com nomes no formato:

NOME_DO_PDF_saida_phi3_mini.json
NOME_DO_PDF_questoes_phi3_mini.txt

ğŸ’¡ ObservaÃ§Ãµes

    O template usado para guiar os modelos estÃ¡ no arquivo template.json.

    Certifique-se de que o caminho para template.json estÃ¡ correto no cÃ³digo.

    Ã‰ possÃ­vel adaptar a lÃ³gica para processar outros tipos de conteÃºdo nÃ£o educacional.
